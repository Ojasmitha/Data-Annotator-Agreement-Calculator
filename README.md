Data Annotator Agreement Calculator
This repository contains a Jupyter Notebook for calculating the agreement between different data annotators using Kappa scores and generating confusion matrices. The purpose of this tool is to assess the reliability of annotations provided by multiple annotators.

Features
Calculate Kappa scores to evaluate inter-annotator agreement.
Generate confusion matrices for visual representation of annotation differences.
Save confusion matrices as PNG files.
Requirements
Python 3.x
pandas
scikit-learn
seaborn
matplotlib
